{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4acc420-4e7e-4557-b205-f1e2d9f4c56c",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> â€¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> â€¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> â€¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# Fine-tune and deploy the multimodal LLaVA model with DeepSpeedðŸ¤™\n",
    "\n",
    "Hi everyone!\n",
    "\n",
    "In this notebook we'll fine-tune the LLaVA model. LLaVA is multimodal which means it can ingest and understand images along with text! LLaVA comes from a research paper titled [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) and introduces the Large Language and Vision Assistant methodology. In order to process images, LLaVA relies on the pre-trained CLIP visual encoder ViT-L/14 which maps images and text into the same latent space. \n",
    "\n",
    "Help us make this tutorial better! Please provide feedback on the [Discord channel](https://discord.gg/RN2a436M73) or on [X](https://x.com/brevdev)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5808bde-44da-4588-95f4-68c292cf9400",
   "metadata": {},
   "source": [
    "## Table of contents \n",
    "\n",
    "1. Data Preprocessing\n",
    "2. LLaVA Installation\n",
    "3. DeepSpeed configuration\n",
    "4. Weights and Biases\n",
    "5. Finetuning flow\n",
    "6. Deployment via gradio interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2638c4-fea4-451d-a512-4e2c731cbdec",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "LLaVA requires data to be in a very specific format. Below we use a [helper function](https://wandb.ai/byyoung3/ml-news/reports/How-to-Fine-Tune-LLaVA-on-a-Custom-Dataset--Vmlldzo2NjUwNTc1) to format the OKV-QA dataset. This dataset teaches the model to respond to an image in short phrases without any preamble or extra verbiage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3d7b07-4e3d-4255-a940-7b5820b100e1",
   "metadata": {},
   "source": [
    "## Back to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290ef4f1-c885-4267-8491-375a89bd243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import data here\n",
    "!pip install gdown\n",
    "!gdown --id 1KaK3iv23ULq0rFfF_OvN9XCgAjic6VZz #HAM10000_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6d71e-8005-479c-b04d-33b1bca4eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unzip data here\n",
    "from zipfile import ZipFile\n",
    "file_name = \"HAM10000_balanced.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cf7a70-0294-4a75-bffe-d0c375ce11ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install preprocessing libraries\n",
    "!pip install datasets\n",
    "!pip install --upgrade --force-reinstall Pillow\n",
    "#this was added\n",
    "!pip install torch==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1078bcba-0a3f-4818-a745-2f3cbdff7c3a",
   "metadata": {},
   "source": [
    "## Install LLaVA\n",
    "\n",
    "To install the functions needed to use the model, we have to clone the original LLaVA repository and and install it in editable mode. This lets us access all functions and helper methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0387008-d4ad-4c58-bb9d-d77f71637257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The pip install -e . lets us install the repository in editable mode\n",
    "!git clone https://github.com/haotian-liu/LLaVA.git\n",
    "!cd LLaVA && pip install --upgrade pip && pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa03b91-85a8-434a-81f6-105dfa87ecb2",
   "metadata": {},
   "source": [
    "## DeepSpeed\n",
    "\n",
    "Microsoft DeepSpeed is a deep learning optimization library designed to enhance the training speed and scalability of large-scale artificial intelligence (AI) models. Developed by Microsoft, this open-source tool specifically addresses the challenges associated with training very large models, allowing for reduced computational times and resource usage. By optimizing memory management and introducing novel parallelism techniques, DeepSpeed enables developers and researchers to train models with billions of parameters efficiently, even on limited hardware setups.DeepSpeed API is a lightweight wrapper on PyTorch. DeepSpeed manages all of the boilerplate training techniques, such as distributed training, mixed precision, gradient accumulation, and checkpoints and allows you to just focus on model development. To learn more about DeepSpeed and how it performs the magic, check out this [article](https://www.deepspeed.ai/2021/03/07/zero3-offload.html) on DeepSpeed and ZeRO.\n",
    "\n",
    "Using deepspeed is extremely simple - you simply pip install it! The LLaVA respository contains the setup scripts and configuration files needed to finetune in different ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55721ad3-f88f-4e03-9d99-d193c276bd0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd LLaVA && pip install -e \".[train]\"\n",
    "#version specified here\n",
    "!pip install flash-attn==2.7.3 --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d16e09-a3ec-461b-8a95-78c3a4e22379",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ac37a-10ad-46b9-84f7-ef289a29bbbb",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "Weights and Biases is an industry standard MLOps tool to used to monitor and evaluate training jobs. At Brev, we use Weights and Biases to track all of our finetuning jobs! Its extremely easy to setup and plugs into the DeepSpeed training loop. You simply create an account and use the cells below to log in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae96a8-f281-471c-aeb9-dac6bc7f5bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f3d47-8daa-480a-acd7-6517bac50c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c575a4-55ea-4af2-b294-d813424f6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was added for next cell to work\n",
    "!pip install peft==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc74b4e-6c88-4e9e-92bd-610a54fa01a3",
   "metadata": {},
   "source": [
    "## Finetuning job\n",
    "\n",
    "Below we start the DeepSpeed training run for 5 epochs. It will automatically recognize multiple GPUs and parallelize across them. Most of the input flags are standard but you can adjust your training run with the `num_train_epochs` and `per_device_train_batch_size` flags!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2019238-2ce5-4985-98c1-7dabf9c10169",
   "metadata": {},
   "outputs": [],
   "source": [
    "!deepspeed LLaVA/llava/train/train_mem.py \\\n",
    "    --lora_enable True --lora_r 128 --lora_alpha 256 --lora_dropout 0.05 --mm_projector_lr 2e-5 \\\n",
    "    --deepspeed LLaVA/scripts/zero3.json \\\n",
    "    --model_name_or_path liuhaotian/llava-v1.5-13b \\\n",
    "    --version v1 \\\n",
    "    --data_path ./dataset/train/dataset.json \\\n",
    "    --image_folder ./dataset/images \\\n",
    "    --vision_tower openai/clip-vit-large-patch14-336 \\\n",
    "    --mm_projector_type mlp2x_gelu \\\n",
    "    --mm_vision_select_layer -2 \\\n",
    "    --mm_use_im_start_end False \\\n",
    "    --mm_use_im_patch_token False \\\n",
    "    --image_aspect_ratio pad \\\n",
    "    --group_by_modality_length True \\\n",
    "    --bf16 True \\\n",
    "    --output_dir ./checkpoints/llava-v1.5-13b-task-lora \\\n",
    "    --num_train_epochs 5 \\\n",
    "    --per_device_train_batch_size 16 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 50000 \\\n",
    "    --save_total_limit 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --lazy_preprocess True \\\n",
    "    --report_to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657dca5d-8014-4c28-a922-c24187d9db2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the LoRA weights with the full model\n",
    "!python LLaVA/scripts/merge_lora_weights.py --model-path checkpoints/llava-v1.5-13b-task-lora --model-base liuhaotian/llava-v1.5-13b --save-model-path llava-ftmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e52c548-a1b4-477a-ad68-1d9ac153fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bump transformers down for gradio/deployment inference if needed\n",
    "!pip install transformers==4.37.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f3d67-3c9c-4413-b86d-0a4ec2883df8",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "LLaVA gives us 2 ways to deploy the model - via CLI or Gradio UI. We suggest using the Gradio UI for interactivity as you can compare two models and see the finetuning effect compared to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34046faf-3e2c-4726-9e41-6f4bb3028dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the lines below to run the CLI. You need to pass in a JPG image URL to use the multimodal capabilities\n",
    "\n",
    "# !python -m llava.serve.cli \\\n",
    "#     --model-path llava-ftmodel \\\n",
    "#     --image-file \"https://llava-vl.github.io/static/images/view.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a9a5a-d386-4b9b-a9f6-808217d6f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model runner\n",
    "!wget -L https://raw.githubusercontent.com/brevdev/notebooks/main/assets/llava-deploy.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae645e95-d301-4f8a-918e-07cdfba043bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was added for UI to work\n",
    "%cd LLaVA \n",
    "!pip install gradio -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319cff98-f9c1-4241-9831-e56548b592fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#back to home directory\n",
    "%cd /home/ubuntu/verb-workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9550ef4-e881-49d7-a237-a997234d179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference! Use the public link provided in the output to test\n",
    "!chmod +x llava-deploy.sh && ./llava-deploy.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b04bfa-3bce-4e64-922d-631450e02c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
